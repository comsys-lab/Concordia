The number of layers for each model.
PaLM        : 118
GPT3        : 96
Chinchilla  : 80
T5          : 24 for encoder, 24 for decoder
BERT Large  : 24